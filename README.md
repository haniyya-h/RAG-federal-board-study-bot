# ğŸ“š Federal Board Study Bot - RAG Implementation

A RAG (Retrieval-Augmented Generation) based chatbot for Federal Board students in Pakistan. This project demonstrates the complete implementation of a RAG system using modern AI technologies including vector databases, embeddings, and large language models.

## ğŸ¯ Assignment Overview

This project implements a complete RAG system that:
- **Processes PDF documents** using OCR technology
- **Creates vector embeddings** using Google Gemini AI
- **Stores embeddings** in ChromaDB vector database
- **Retrieves relevant content** based on user queries
- **Generates answers** using Google Gemini LLM
- **Provides practice questions** dynamically generated by AI
- **Supports multiple subjects** across Grades 9-12

## ğŸ—ï¸ RAG Architecture

### 1. **Document Ingestion Pipeline**
```
PDF Documents â†’ OCR Processing â†’ Text Chunking â†’ Embedding Generation â†’ Vector Storage
```

- **PDF Processing**: Uses PyMuPDF and PyTesseract for OCR
- **Text Chunking**: RecursiveCharacterTextSplitter (800 chars, 100 overlap)
- **Embeddings**: Google Gemini embedding-001 model
- **Vector Storage**: ChromaDB with persistent storage

### 2. **Query Processing Pipeline**
```
User Query â†’ Vector Search â†’ Context Retrieval â†’ LLM Generation â†’ Response Formatting
```

- **Vector Search**: Semantic similarity search in ChromaDB
- **Context Retrieval**: Top-k relevant chunks (k=3)
- **LLM Generation**: Google Gemini 1.5-flash model
- **Response Formatting**: Structured output with sources and practice questions

### 3. **RAG Components**
- **Retriever**: ChromaDB vector store with similarity search
- **Generator**: Google Gemini LLM with custom prompts
- **Context**: Retrieved document chunks with metadata
- **Output**: Answer + sources + practice questions

## ğŸ› ï¸ Technical Implementation

### **RAG Framework**
- **LangChain** - LLM application framework and chains
- **RetrievalQA Chain** - Combines retriever and generator
- **Custom Prompts** - Tailored for student-friendly responses

### **Vector Database**
- **ChromaDB** - Persistent vector storage
- **Collection Management** - Separate collections per grade/subject
- **Similarity Search** - Cosine similarity with configurable k

### **AI Models**
- **Google Gemini 1.5-flash** - Language model for generation
- **Google Gemini embedding-001** - Text embeddings
- **Temperature: 0.1** - Consistent, factual responses

### **Document Processing**
- **PyMuPDF (fitz)** - PDF to image conversion
- **PyTesseract** - OCR with multiple PSM modes
- **Pillow (PIL)** - Image preprocessing
- **RecursiveCharacterTextSplitter** - Text chunking strategy

## ğŸš€ Setup Instructions

### **Prerequisites**
1. **Python 3.12+**
2. **Tesseract OCR** - [Installation Guide](https://tesseract-ocr.github.io/tessdoc/Installation.html)
3. **Google Gemini API Key** - [Get Free Key](https://aistudio.google.com/)

### **Installation**
```bash
# Clone repository
git clone https://github.com/haniyya-h/RAG-federal-board-study-bot.git
cd RAG-federal-board-study-bot

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env and add your Google API key
```

### **Data Processing**
```bash
# Add PDFs to data/grade_X/ folders
# Run preprocessing
python preprocess.py
```

### **Run Application**
```bash
streamlit run app.py
```

## ğŸ“Š RAG Performance Metrics

### **Processing Statistics**
- **PDF Processing**: ~2-5 seconds per page (OCR)
- **Embedding Generation**: ~1-2 seconds per chunk
- **Vector Storage**: ~100-500MB per grade/subject
- **Query Response**: ~3-5 seconds per question

### **Accuracy Metrics**
- **OCR Accuracy**: ~85-95% (depends on PDF quality)
- **Retrieval Relevance**: Top-3 chunks with cosine similarity >0.7
- **Answer Quality**: Contextual and source-grounded responses
- **Question Generation**: 3 relevant practice questions per query

## ğŸ”¬ Technical Features

### **Advanced OCR Processing**
- **Multiple PSM Modes**: Optimized for different text layouts
- **Zoom Levels**: 1.5x, 2.0x, 3.0x for better accuracy
- **Image Preprocessing**: Contrast enhancement and noise reduction
- **Fallback Strategies**: Multiple attempts for difficult pages

### **Intelligent Chunking**
- **Overlap Strategy**: 100-character overlap for context preservation
- **Metadata Preservation**: Page numbers, chapters, and source info
- **Size Optimization**: 800-character chunks for optimal retrieval

### **Dynamic Question Generation**
- **Context-Aware**: Questions based on retrieved content
- **Grade-Appropriate**: Difficulty matched to student level
- **SLO-Style**: Student Learning Outcome format
- **Fallback System**: General questions if AI generation fails

## ğŸ“ Project Structure

```
RAG-federal-board-study-bot/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ preprocess.py          # PDF processing and embedding creation
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .gitignore            # Git ignore rules
â”œâ”€â”€ README.md             # Project documentation
â”œâ”€â”€ .env.example          # Environment template
â”œâ”€â”€ data/                 # PDF textbooks directory (user-provided)
â”‚   â”œâ”€â”€ grade_9/
â”‚   â”œâ”€â”€ grade_10/
â”‚   â”œâ”€â”€ grade_11/
â”‚   â””â”€â”€ grade_12/
â””â”€â”€ embeddings/           # ChromaDB vector stores (generated)
    â”œâ”€â”€ grade_9_mathematics/
    â”œâ”€â”€ grade_9_biology/
    â””â”€â”€ ...
```

## ğŸ¯ RAG Implementation Features

### **Document Processing**
- âœ… **OCR Integration** - Handles scanned PDF documents
- âœ… **Intelligent Chunking** - Optimized text splitting with overlap
- âœ… **Metadata Preservation** - Page numbers, chapters, source tracking
- âœ… **Batch Processing** - Handles multiple documents efficiently

### **Vector Database**
- âœ… **ChromaDB Integration** - Persistent vector storage
- âœ… **Collection Management** - Organized by grade and subject
- âœ… **Similarity Search** - Cosine similarity with configurable parameters
- âœ… **Metadata Filtering** - Context-aware retrieval

### **AI Integration**
- âœ… **Google Gemini LLM** - State-of-the-art language model
- âœ… **Custom Prompts** - Tailored for educational responses
- âœ… **Dynamic Question Generation** - AI-generated practice questions
- âœ… **Context-Aware Responses** - Grounded in retrieved documents

### **User Interface**
- âœ… **Streamlit Web App** - Modern, responsive interface
- âœ… **Multi-Grade Support** - Grades 9-12
- âœ… **Multi-Subject Support** - All Federal Board subjects
- âœ… **Real-time Processing** - Live question answering

## ğŸ”§ RAG Configuration

### **Vector Database Settings**
```python
# ChromaDB configuration
CHUNK_SIZE = 800          # Text chunk size
CHUNK_OVERLAP = 100       # Overlap between chunks
TOP_K = 3                 # Number of chunks to retrieve
SIMILARITY_THRESHOLD = 0.7 # Minimum similarity score
```

### **AI Model Settings**
```python
# Google Gemini configuration
MODEL_NAME = "gemini-1.5-flash"
TEMPERATURE = 0.1         # Low temperature for factual responses
EMBEDDING_MODEL = "models/embedding-001"
```

### **OCR Settings**
```python
# Tesseract configuration
PSM_MODES = [6, 3, 1]     # Page segmentation modes
ZOOM_LEVELS = [1.5, 2.0, 3.0]  # Image scaling for better OCR
```

## ğŸ§ª Testing the RAG System

### **Test Queries**
Try these sample questions to test the RAG implementation:

**Mathematics:**
- "What is the quadratic formula?"
- "How do you solve linear equations?"
- "Explain the Pythagorean theorem"

**Biology:**
- "What is photosynthesis?"
- "Describe the structure of a cell"
- "How does DNA replication work?"

**Physics:**
- "What is Newton's first law?"
- "Explain the concept of force"
- "How does electricity work?"

### **Expected Outputs**
- **Answer**: Contextual response from textbook
- **Sources**: Page numbers and chapter references
- **Practice Questions**: 3 AI-generated questions
- **Response Time**: 3-5 seconds per query

## ğŸ“ˆ RAG Evaluation

### **Retrieval Quality**
- **Precision**: Relevant chunks retrieved
- **Recall**: Complete coverage of query topic
- **Relevance**: Semantic similarity to user query

### **Generation Quality**
- **Accuracy**: Factual correctness of responses
- **Relevance**: Directly answers user question
- **Completeness**: Comprehensive coverage of topic

### **System Performance**
- **Latency**: End-to-end response time
- **Throughput**: Queries processed per minute
- **Scalability**: Performance with larger datasets

## ğŸ“ Assignment Learning Outcomes

This project demonstrates:

1. **RAG Architecture** - Complete implementation of retrieval-augmented generation
2. **Vector Databases** - ChromaDB for semantic search and storage
3. **Document Processing** - OCR, chunking, and embedding generation
4. **AI Integration** - LLM integration with custom prompts
5. **Web Development** - Streamlit for user interface
6. **Python Programming** - Modern Python with clean, modular code
7. **API Integration** - Google Gemini AI services
8. **Data Pipeline** - End-to-end document processing workflow

## ğŸ“š Technical References

- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Google Gemini API](https://ai.google.dev/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [RAG Paper](https://arxiv.org/abs/2005.11401)

---

**Made with â¤ï¸ for Federal Board students in Pakistan**

*Empowering students with AI-powered learning tools* ğŸš€
